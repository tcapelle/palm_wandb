{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Palm and W&B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to setup Palm access and the corresponding VertexAI service account. \n",
    "\n",
    "See the documentation on how to get started using [Generative AI on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will  need to setup the GCP variables related to your project and zone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"wandb-growth\"\n",
    "zone = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get started with this example form the [PALM](https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-text) documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interview(\n",
    "    temperature: float,\n",
    "    project_id: str = project_id,\n",
    "    location: str = zone,\n",
    ") -> str:\n",
    "    \"\"\"Ideation example with a Large Language Model\"\"\"\n",
    "\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    # TODO developer - override these parameters as needed:\n",
    "    parameters = {\n",
    "        \"temperature\": temperature,  # Temperature controls the degree of randomness in token selection.\n",
    "        \"max_output_tokens\": 256,  # Token limit determines the maximum amount of text output.\n",
    "        \"top_p\": 0.8,  # Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\n",
    "        \"top_k\": 40,  # A top_k of 1 means the selected token is the most probable among all tokens.\n",
    "    }\n",
    "\n",
    "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "    response = model.predict(\n",
    "        \"Give me ten interview questions for the role of program manager.\",\n",
    "        **parameters,\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is your experience with project management?\n",
      "2. What is your process for managing a project?\n",
      "3. How do you handle unexpected challenges or roadblocks?\n",
      "4. What are your communication skills like?\n",
      "5. How do you work with stakeholders?\n",
      "6. How do you manage your time effectively?\n",
      "7. How do you stay organized?\n",
      "8. What are your strengths and weaknesses as a project manager?\n",
      "9. What are your salary expectations?\n",
      "10. Why are you interested in this position?\n"
     ]
    }
   ],
   "source": [
    "res = interview(0.7)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use Weights and Biases Tables to store our model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use W&B Tables and keep track of the model's generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "queries = [\n",
    "    \"The planet earth is the \",\n",
    "    \"Implement a Python function to compute the Fibonacci numbers.\",\n",
    "    \"Write a Rust function that performs binary exponentiation.\",\n",
    "    \"How do I allocate memory in C?\",\n",
    "    \"What are the differences between Javascript and Python?\",\n",
    "    \"How do I find invalid indices in Postgres?\",\n",
    "    \"How can you implement a LRU (Least Recently Used) cache in Python?\",\n",
    "    \"What approach would you use to detect and prevent race conditions in a multithreaded application?\",\n",
    "    \"Can you explain how a decision tree algorithm works in machine learning?\",\n",
    "    \"How would you design a simple key-value store database from scratch?\",\n",
    "    \"How do you handle deadlock situations in concurrent programming?\",\n",
    "    \"What is the logic behind the A* search algorithm, and where is it used?\",\n",
    "    \"How can you design an efficient autocomplete system?\",\n",
    "    \"What approach would you take to design a secure session management system in a web application?\",\n",
    "    \"How would you handle collision in a hash table?\",\n",
    "    \"How can you implement a load balancer for a distributed system?\",\n",
    "    \"What is the fable involving a fox and grapes?\",\n",
    "    \"Write a story in the style of James Joyce about a trip to the Australian outback in 2083, to see robots in the beautiful desert.\",\n",
    "    \"Who does Harry turn into a balloon?\",\n",
    "    \"Write a tale about a time-traveling historian who's determined to witness the most significant events in human history.\",\n",
    "    \"Describe a day in the life of a secret agent who's also a full-time parent.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refactor the call function so it takes arbitrary prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def palm_call(\n",
    "    prompt: str,\n",
    "    temperature: float = 0.7,\n",
    "    max_output_tokens: int = 256,\n",
    "    top_p: float = 0.8,\n",
    "    top_k: int = 40,\n",
    "    project_id: str = project_id,\n",
    "    location: str = zone,\n",
    ") -> str:\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    parameters = {\n",
    "        \"temperature\": temperature,  # Temperature controls the degree of randomness in token selection.\n",
    "        \"max_output_tokens\": max_output_tokens,  # Token limit determines the maximum amount of text output.\n",
    "        \"top_p\": top_p,  # Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\n",
    "        \"top_k\": top_k,  # A top_k of 1 means the selected token is the most probable among all tokens.\n",
    "    }\n",
    "\n",
    "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "    response = model.predict(\n",
    "        prompt,\n",
    "        **parameters,\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a `wandb.Table` with the corresponding columns we want to keep track of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = wandb.Table(columns=[\"model\", \"time\", \"temperature\", \"max_output_tokens\", \"top_p\", \"top_k\", \"prompt\", \"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:54<00:00,  2.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# let's define some configuration parameters\n",
    "config = dict(\n",
    "    temperature = 1.0,\n",
    "    max_output_tokens = 128,\n",
    "    top_p = 0.8,\n",
    "    top_k = 40,\n",
    ")\n",
    "\n",
    "# we iterate through the queries and call the model\n",
    "# adding the results to a table\n",
    "for q in tqdm(queries):\n",
    "    t0 = time.perf_counter()\n",
    "    res = palm_call(q, **config)\n",
    "    table.add_data(\n",
    "        \"text-bison@001\", \n",
    "        time.perf_counter() - t0, \n",
    "        config[\"temperature\"], \n",
    "        config[\"max_output_tokens\"], \n",
    "        config[\"top_p\"], \n",
    "        config[\"top_k\"], \n",
    "        q, \n",
    "        res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the table to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tcapelle/work/palm/wandb/run-20230824_164052-mvrpbvdr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/wandb-palm/runs/mvrpbvdr' target=\"_blank\">stilted-moon-5</a></strong> to <a href='https://wandb.ai/capecape/wandb-palm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/wandb-palm' target=\"_blank\">https://wandb.ai/capecape/wandb-palm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/wandb-palm/runs/mvrpbvdr' target=\"_blank\">https://wandb.ai/capecape/wandb-palm/runs/mvrpbvdr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stilted-moon-5</strong> at: <a href='https://wandb.ai/capecape/wandb-palm/runs/mvrpbvdr' target=\"_blank\">https://wandb.ai/capecape/wandb-palm/runs/mvrpbvdr</a><br/> View job at <a href='https://wandb.ai/capecape/wandb-palm/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkyNTk1MjI1/version_details/v0' target=\"_blank\">https://wandb.ai/capecape/wandb-palm/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkyNTk1MjI1/version_details/v0</a><br/>Synced 6 W&B file(s), 1 media file(s), 6 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230824_164052-mvrpbvdr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"wandb-palm\", config=config)\n",
    "\n",
    "wandb.log({\"palm_samples\": table})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Integration\n",
    "\n",
    "Weights and Biases supports Langchain, this means that you can use Palm in your Langchain application and benefit from the full suite of features that Weights and Biases offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time, wandb\n",
    "\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.tools import BaseTool\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:scoayhx1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tcapelle/work/palm/wandb/run-20230824_143151-rqy9ksjd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/wandb-palm/runs/rqy9ksjd' target=\"_blank\">silver-river-5</a></strong> to <a href='https://wandb.ai/capecape/wandb-palm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/wandb-palm' target=\"_blank\">https://wandb.ai/capecape/wandb-palm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/wandb-palm/runs/rqy9ksjd' target=\"_blank\">https://wandb.ai/capecape/wandb-palm/runs/rqy9ksjd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/capecape/wandb-palm/runs/rqy9ksjd?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x286dccf10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"wandb-palm\", job_type=\"generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = VertexAI(model=\"text-bison@001\", project=\"wandb-growth\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldPickerTool(BaseTool):\n",
    "    name = \"pick_world\"\n",
    "    description = \"pick a virtual game world for your character or item naming\"\n",
    "    worlds = [\n",
    "                \"a mystic medieval island inhabited by intelligent and funny frogs\",\n",
    "                \"a modern anthill featuring a cyber-ant queen and her cyber-ant-workers\",\n",
    "                \"a digital world inhabited by friendly machine learning engineers\"\n",
    "            ]\n",
    "\n",
    "    def _run(\n",
    "        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        time.sleep(1)\n",
    "        return random.choice(self.worlds)\n",
    "\n",
    "    async def _arun(\n",
    "        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"pick_world does not support async\")\n",
    "        \n",
    "class NameValidatorTool(BaseTool):\n",
    "    name = \"validate_name\"\n",
    "    description = \"validate if the name is properly generated\"\n",
    "\n",
    "    def _run(\n",
    "        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        time.sleep(1)\n",
    "        if len(query) < 20:\n",
    "            return f\"This is a correct name: {query}\"\n",
    "        else:\n",
    "            return f\"This name is too long. It should be shorter than 20 characters.\"\n",
    "\n",
    "    async def _arun(\n",
    "        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"validate_name does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [WorldPickerTool(), NameValidatorTool()]\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m pick a virtual game world\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma modern anthill featuring a cyber-ant queen and her cyber-ant-workers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick a name for the hero\n",
      "Action: pick_name\n",
      "Action Input: None\u001b[0m\n",
      "Observation: pick_name is not a valid tool, try one of [pick_world, validate_name].\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick a name for the hero\n",
      "Action: pick_name\n",
      "Action Input: None\u001b[0m\n",
      "Observation: pick_name is not a valid tool, try one of [pick_world, validate_name].\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick a name for the hero\n",
      "Action: validate_name\n",
      "Action Input: \"Cyber-ant Queen\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mThis is a correct name: Cyber-ant Queen\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Cyber-ant Queen in a modern anthill featuring a cyber-ant queen and her cyber-ant-workers\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cyber-ant Queen in a modern anthill featuring a cyber-ant queen and her cyber-ant-workers'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\n",
    "    \"Find a virtual game world for me and imagine the name of a hero in that world\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m pick_world\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma modern anthill featuring a cyber-ant queen and her cyber-ant-workers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick_world\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma modern anthill featuring a cyber-ant queen and her cyber-ant-workers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick_world\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma modern anthill featuring a cyber-ant queen and her cyber-ant-workers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick_world\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma digital world inhabited by friendly machine learning engineers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick_world\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma mystic medieval island inhabited by intelligent and funny frogs\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick_world\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma modern anthill featuring a cyber-ant queen and her cyber-ant-workers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick_world\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma digital world inhabited by friendly machine learning engineers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick_world\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma modern anthill featuring a cyber-ant queen and her cyber-ant-workers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick_world\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma digital world inhabited by friendly machine learning engineers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick_world\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma modern anthill featuring a cyber-ant queen and her cyber-ant-workers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick_world\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma digital world inhabited by friendly machine learning engineers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick_world\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma modern anthill featuring a cyber-ant queen and her cyber-ant-workers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick_world\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma modern anthill featuring a cyber-ant queen and her cyber-ant-workers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick_world\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma modern anthill featuring a cyber-ant queen and her cyber-ant-workers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m pick_world\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma modern anthill featuring a cyber-ant queen and her cyber-ant-workers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Agent stopped due to iteration limit or time limit.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\n",
    "    \"Find a virtual game world for me and imagine the name of a jewel in that world\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Let me pick a virtual game world first.\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma modern anthill featuring a cyber-ant queen and her cyber-ant-workers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Now I need to imagine the name of food in that world.\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma modern anthill featuring a cyber-ant queen and her cyber-ant-workers\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Let me validate the name.\n",
      "Action: validate_name\n",
      "Action Input: cyber-ant-queen-cake\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mThis name is too long. It should be shorter than 20 characters.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Let me try another name.\n",
      "Action: pick_world\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3ma mystic medieval island inhabited by intelligent and funny frogs\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Let me validate the name.\n",
      "Action: validate_name\n",
      "Action Input: frog-pie\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mThis is a correct name: frog-pie\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: frog-pie\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'frog-pie'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\n",
    "    \"Find a virtual game world for me and imagine the name of food in that world.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silver-river-5</strong> at: <a href='https://wandb.ai/capecape/wandb-palm/runs/rqy9ksjd' target=\"_blank\">https://wandb.ai/capecape/wandb-palm/runs/rqy9ksjd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230824_143151-rqy9ksjd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
